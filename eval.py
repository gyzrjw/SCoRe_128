import sys
import os


sys.path.append('../')
sys.path.append('./')

os.environ["VLLM_LOGGING_LEVEL"] = 'FATAL'

import argparse
from pathlib import Path
import datasets
from datasets import Dataset, DatasetDict
from functools import partial
from vllm import LLM, SamplingParams
from utils.generation_utils import generate_for_dataset, store_generation_results, load_config
from prompts.prompt_schemas import load_few_shot_prompts
from utils.eval_utils import RewardEvaluator
from utils.utils import KM, flatten_predictions
from transformers import AutoTokenizer
import yaml
import subprocess
import torch
import logging
from prompts import get_prompt_builder


def collect_correction_stats(
    dataset,
    reward_function,
    question_col="question",
    reference_col="reference",
    inital_answer_col="star_correction_initial_generation",
    correction_col="star_correction",
):
    """
    Computes statistics on corrections as percentages:
    - Percentage of samples that went from Correct to Incorrect
    - Percentage of samples that remained Correct
    - Percentage of samples that improved from Incorrect to Correct

    Returns:
        A dictionary with percentages for each category.
    """
    
    correct_to_incorrect = 0
    correct_to_correct = 0
    incorrect_to_correct = 0
    total_corrections = 0

    # Iterate over each row in the dataset
    for row in dataset:
        reference = row[reference_col]

        # Check initial correctness
        initial_answers = row[inital_answer_col]
        corrected_answers = row[correction_col]

        for init_answer in initial_answers:
            init_is_correct = reward_function(ground_truth=reference, model_answer=init_answer)

            for correction in flatten_predictions(corrected_answers):
                correction_is_correct = reward_function(ground_truth=reference, model_answer=correction)

                total_corrections += 1

                if init_is_correct and not correction_is_correct:
                    correct_to_incorrect += 1
                elif init_is_correct and correction_is_correct:
                    correct_to_correct += 1
                elif not init_is_correct and correction_is_correct:
                    incorrect_to_correct += 1

    # Avoid division by zero
    if total_corrections == 0:
        stats = {
            "correct_to_incorrect": 0.0,
            "correct_to_correct": 0.0,
            "incorrect_to_correct": 0.0
        }
    else:
        stats = {
            "correct_to_incorrect": (correct_to_incorrect / total_corrections) * 100,
            "correct_to_correct": (correct_to_correct / total_corrections) * 100,
            "incorrect_to_correct": (incorrect_to_correct / total_corrections) * 100
        }


    return stats



def perform_generation(data, model, prompt_func, sampling_params, id_key, output_col):
    """
    Perform (rationale) generation or (rationalization) generation for the dataset.
    Store the generation results in the dataset under 'output_col'.
    """
    generation_results = generate_for_dataset(
        model=model,
        data=data,
        prompt_function=prompt_func,
        sampling_params=sampling_params,
        id_key=id_key
    )
    return store_generation_results(data, generation_results, result_col=output_col, id_col=id_key)

# --- Generation Helper ---

def run_generation_phase(data_train, data_test, model, prompt_func, output_col, num_outputs, id_key, config, phase_label, sampling_params, **kwargs):
    """
    Sets the sampling parameter, performs generation on train and test splits,
    computes accuracy via KM, and prints results.
    Returns updated train and test datasets.
    """
    print(f"[INFO] Starting {phase_label} generation...")
    sampling_params.n = 1
    new_test = perform_generation(data_test, model, prompt_func, sampling_params, id_key, output_col, **kwargs)
    
    return new_test

def branch_initial_generation(config, model, train_data, test_data, init_prompt_func, sampling_params, iteration, ft_dataset_path, reward_function):
    new_test = run_generation_phase(
        train_data, test_data, model, init_prompt_func,
        output_col='star_correction_initial_generation',
        num_outputs=1,
        id_key=config['id_col'],
        config=config,
        phase_label="Initial Answer",
        sampling_params=sampling_params,
    )
    print(f"[INFO] Initial Test Accuracy at step {iteration}: {KM(new_test, target_col='star_correction_initial_generation', gt_col=config['gold_col'], evaluator=reward_function)}")
    return new_test

def branch_correction_generation(config, model, train_data, test_data, corr_prompt_func, sampling_params, iteration, ft_dataset_path, reward_function, prompt_builder):
    new_test = run_generation_phase(
        train_data, test_data, model, corr_prompt_func,
        output_col=f'star_correction_{iteration}',
        num_outputs=1,
        id_key=config['id_col'],
        config=config,
        phase_label="Correction",
        sampling_params=sampling_params
    )
    print(f"[INFO] Correction Test Accuracy at step {iteration}: {KM(new_test, target_col=f'star_correction_{iteration}', gt_col=config['gold_col'], evaluator=reward_function)}")

    stats_test = collect_correction_stats(
        dataset=new_test,
        question_col=config['question_col'],
        reference_col=config['gold_col'],
        inital_answer_col='star_correction_initial_generation',
        correction_col=f'star_correction_{iteration}',
        reward_function=reward_function
    )

    print(
        f"[INFO] Test Correction Statistics at step {iteration}:\n"
        f"[INFO]       - Correct → Incorrect: {stats_test['correct_to_incorrect']:.2f}%\n"
        f"[INFO]       - Correct → Correct: {stats_test['correct_to_correct']:.2f}%\n"
        f"[INFO]       - Incorrect → Correct: {stats_test['incorrect_to_correct']:.2f}%"
    )
    new_test.save_to_disk(f"{ft_dataset_path}_test")

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--config_path", type=str, required=True)
    parser.add_argument("--generation_model_path", type=str, required=True)
    parser.add_argument("--ft_dataset_path", type=str, required=True)
    parser.add_argument("--iteration", type=int, required=True)
    args = parser.parse_args()

    iteration = args.iteration

    config = load_config(args.config_path)

    # Load dataset
    dataset = datasets.load_from_disk(str(config['data_path']))
    train_data, test_data = dataset["train"], dataset["test"]


    tokenizer = AutoTokenizer.from_pretrained(config['model_path'], cache_dir=config['cache_dir'])
    prompt_builder = get_prompt_builder(config['task_type'])
    reward_function = RewardEvaluator(config)


    initial_generation_few_shot = load_few_shot_prompts(config['few_shot_dir'], f"{config['task_type']}_initial")
    correction_few_shot = load_few_shot_prompts(config['few_shot_dir'], f"{config['task_type']}_correction")
    

    # Prompt functions
    initial_generation_prompt_func = partial(
        prompt_builder.build_initial_generation_prompt,
        tokenizer=tokenizer,
        question_col=config['question_col'],
        few_shot_prompts=initial_generation_few_shot,
    )

    correction_prompt_func = partial(
        prompt_builder.build_correction_prompt,
        tokenizer=tokenizer,
        question_col=config['question_col'],
        few_shot_prompts=correction_few_shot,
        initial_answer_col='star_correction_initial_generation'
    )

    print(f"Generating from Model {args.generation_model_path}")
    

    # Initialize model (M0)
    model = LLM(
        args.generation_model_path,
        download_dir=config['cache_dir'],
        dtype=torch.bfloat16,
        tensor_parallel_size=torch.cuda.device_count(),
        gpu_memory_utilization=0.95,
        enforce_eager=False,
        max_model_len=config['max_tokens'] * 3,
        disable_log_stats=True,  # Disables logging statistics
        seed=config['random_seed']
        #disable_log_requests=True,  # Disables logging requests
    )
    # Sampling parameters
    sampling_params = SamplingParams(
        temperature=config['temperature'],
        top_p=1.0,
        max_tokens=config['max_tokens'],
        n=1,
        seed=config['random_seed'],
    )

    new_test = branch_initial_generation(
                config=config, 
                model=model, 
                train_data=train_data, 
                test_data=test_data, 
                init_prompt_func=initial_generation_prompt_func,
                sampling_params=sampling_params, 
                iteration=iteration,
                ft_dataset_path=args.ft_dataset_path,
                reward_function=reward_function
                )
    branch_correction_generation(config, model, None, new_test, correction_prompt_func, sampling_params, iteration, args.ft_dataset_path,
        reward_function=reward_function, prompt_builder=prompt_builder)


if __name__ == '__main__':
    main()
