import gc
import math
import os
import time
from collections import defaultdict
from typing import Callable, Optional, Union

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from accelerate import Accelerator
from accelerate.utils import broadcast, gather_object
from datasets import Dataset
from torch.utils.data import DataLoader
from transformers import (
    BaseImageProcessor,
    DataCollatorWithPadding,
    FeatureExtractionMixin,
    GenerationConfig,
    PreTrainedTokenizerBase,
    ProcessorMixin,
    Trainer,
    TrainerCallback,
    TrainerControl,
    get_cosine_schedule_with_warmup
)
from transformers.integrations import get_reporting_integration_callbacks
from transformers.trainer import DEFAULT_CALLBACKS, DEFAULT_PROGRESS_CALLBACK
from transformers.trainer_callback import CallbackHandler, ExportableState, PrinterCallback

# Assume these come from TRLâ€™s code (as in RLOOTrainer):
from trl.models.utils import unwrap_model_for_generation
from trl.trainer.utils import (
    OnlineTrainerState,
    batch_generation,
    disable_dropout_in_model,
    exact_div,
    first_true_indices,
    forward,
    get_reward,
    prepare_deepspeed,
    print_rich_table,
    selective_log_softmax,
    truncate_response,
)
from trl.trainer.rloo_config import RLOOConfig  # or create a new SCoREConfig
from trl.trainer.utils import generate_model_card, get_comet_experiment_url, log_table_to_comet_experiment
from torch.distributed.fsdp import FullyShardedDataParallel as FSDP
from copy import deepcopy



# TODO: Add Scheduler, add validation

INVALID_LOGPROB = 1.0

class SCoRETrainer(Trainer):
    """
    A single-stage SCoRE algorithm implemented in the style of TRL's RLOOTrainer.

    å…ˆç”Ÿæˆ initial answer (y1) â€” å¹¶è®¡ç®—ä¸ ref policy çš„ KL çº¦æŸ
    å†ç”Ÿæˆ correction (y2) â€” è®¡ç®— reward (r2)
    æœ€ç»ˆç›®æ ‡ï¼šR = r2 - beta * KL(y1)ï¼ˆStage 1ï¼‰ æˆ– Stage 2 çš„æ›´å¤æ‚é…æ–¹
    ä½¿ç”¨ REINFORCE (policy gradient) å¯¹ y2 çš„ token log-prob åšæ¢¯åº¦æ›´æ–°ï¼ˆå¹¶åœ¨ Stage2 ä¸­åŒæ—¶æ›´æ–° y1ï¼‰

    Key differences from standard PPO/POLICY GRAD approaches:
      - We generate an INITIAL answer (which gets a KL penalty vs. ref policy).
      - Then we generate a CORRECTION from the policy, which gets a reward from
        the reward/cost function.
      - Final scalar = Reward(correction) - beta * KL(initial).
      - Use REINFORCE on the correction tokens to update the policy.

    Everything else (Accelerator, logging, etc.) is kept consistent
    with the RLOOTrainer style.
    """
    _tag_names = ["trl", "score"]  # for optional tracking in your model config

    def __init__(
        self,
        config: RLOOConfig,
        algo_config,
        processing_class: Optional[
            Union[PreTrainedTokenizerBase, BaseImageProcessor, FeatureExtractionMixin, ProcessorMixin]
        ],
        policy: nn.Module,
        ref_policy: nn.Module,
        prompt_builder,
        reward_model: Union[nn.Module, Callable[[list[str]], list[float]]],
        train_dataset: Dataset,
        data_collator: Optional[DataCollatorWithPadding] = None,
        eval_dataset: Optional[Union[Dataset, dict[str, Dataset]]] = None,
        optimizers: tuple[torch.optim.Optimizer, torch.optim.lr_scheduler._LRScheduler] = (None, None),
        callbacks: Optional[list[TrainerCallback]] = None,
    ) -> None:
        """
        Very similar to RLOOTrainer.__init__, except we note that we only do single-step REINFORCE
        logic inside the train loop.
        """
        #ä¿è¯ref_policyä¸ policy ä¸æ˜¯åŒä¸€å¯¹è±¡
        if ref_policy is policy:    
            raise ValueError(
                "`policy` and `ref_policy` cannot be the same object. If you want `ref_policy` to be the "
                "same as `policy`, pass a *copy* or pass `None` if using PEFT's read-only approach."
            )

        # æ˜¾å¼å†»ç»“ ref_policy çš„æ‰€æœ‰å‚æ•°ï¼Œç¡®ä¿ä¸ä¼šè¢«æ›´æ–°
        for param in ref_policy.parameters():
            param.requires_grad = False
        ref_policy.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼

        self.args = config  # For TRL, a config derived from RLOOConfig or similar
        args = config
        self.algo_config = algo_config
        self.processing_class = processing_class
        self.policy = policy
        self.ref_policy = ref_policy
        self.reward_model = reward_model
        self.train_dataset = train_dataset
        self.eval_dataset = eval_dataset
        self.optimizer, self.lr_scheduler = optimizers
        self.optimizer_cls_and_kwargs = None  # used by HF Trainer if re-creating optimizers
        self.prompt_builder = prompt_builder

        
        # å¦‚æœ data_collator æœªç»™å®šï¼Œåˆ™åŸºäº tokenizer åˆ›å»ºé»˜è®¤ collatorï¼›
        # collator ç”¨äºæ•°æ®æ‰“åŒ…ï¼ˆpadding/attention maskï¼‰
        if data_collator is None:
            data_collator = DataCollatorWithPadding(self.processing_class)
        self.data_collator = data_collator

        # ç¦ç”¨ dropout
        for module in [policy, ref_policy, reward_model]:
            if isinstance(module, nn.Module):
                disable_dropout_in_model(module)

        # GenerationConfig è®¾ç½®ï¼ˆinitial / correctionï¼‰
        # ä¸¤è€…é€šå¸¸ç›¸åŒï¼Œå¯æ ¹æ®éœ€è¦è°ƒæ•´
        self.init_generation_config = GenerationConfig(
            max_new_tokens=args.response_length,  # or some separate param, e.g. args.initial_answer_length
            temperature=args.temperature,
            #top_k=0,
            top_p=0.8,
            do_sample=True,
            pad_token_id=None,
            eos_token_id=None,
        )
        self.corr_generation_config = GenerationConfig(
            max_new_tokens=args.response_length,  # or separate param, e.g. correction_length
            temperature=args.temperature,
            #top_k=0,
            top_p=0.8,
            do_sample=True,
            pad_token_id=None,
            eos_token_id=None,
        )

        # Construct the dataloader
        self.train_dataset_len = len(train_dataset)
        if args.total_episodes is None:  # allow user to define episodes in terms of epochs
            args.total_episodes = int(args.num_train_epochs * self.train_dataset_len)

        # Build accelerator
        accelerator = Accelerator()
        self.accelerator = accelerator
        self.accelerator.state.deepspeed_plugin.deepspeed_config['gradient_accumulation_steps'] = self.algo_config['gradient_accumulation_steps']
        args.world_size = accelerator.num_processes

        # This part is from RLOO: computing local_batch_size, micro_batch_size, etc.
        args.local_batch_size = (
            args.per_device_train_batch_size
            * args.gradient_accumulation_steps
        )
        args.micro_batch_size = int(args.per_device_train_batch_size * args.world_size)
        args.batch_size = int(args.local_batch_size * args.world_size)
        # we do not do multiple mini-batches in this example, so skip that part

        # total number of train steps
        args.num_total_batches = math.ceil(args.total_episodes / args.batch_size)
        # name runs etc.
        time_tensor = torch.tensor(int(time.time()), device=accelerator.device)
        time_int = broadcast(time_tensor, 0).item()
        args.run_name = f"{args.exp_name}"

        # Seeds, directories, etc.
        self.local_seed = args.seed
        torch.manual_seed(args.seed)

        # Prepare data loader
        self.dataloader = DataLoader(
            self.train_dataset,
            batch_size=args.local_batch_size,
            shuffle=True,
            collate_fn=self.data_collator,
            drop_last=True,
        )
        self.model = policy  # HF Trainer expects self.model
        self.model, self.optimizer, self.dataloader = accelerator.prepare(self.model, self.optimizer, self.dataloader)
        self.lr_scheduler = get_cosine_schedule_with_warmup(self.optimizer, num_warmup_steps=self.algo_config['num_warmup_steps'], num_training_steps=args.num_total_batches)


        # reset local seed
        torch.manual_seed(self.local_seed)

        # Prepare eval dataloader if needed
        if self.eval_dataset is not None:
            self.eval_dataloader = DataLoader(
                self.eval_dataset,
                batch_size=args.per_device_eval_batch_size,
                collate_fn=self.data_collator,
                drop_last=True,
            )
            self.eval_dataloader = accelerator.prepare(self.eval_dataloader)
        else:
            self.eval_dataloader = None

        # If using DeepSpeed / FSDP
        self.is_deepspeed_enabled = getattr(self.accelerator.state, "deepspeed_plugin", None) is not None
        self.is_fsdp_enabled = getattr(self.accelerator.state, "fsdp_plugin", None) is not None
        if self.is_deepspeed_enabled:
            if isinstance(self.reward_model, nn.Module):
                self.reward_model = prepare_deepspeed(
                    self.reward_model, args.per_device_train_batch_size, args.fp16, args.bf16
                )
            self.ref_policy = prepare_deepspeed(
                self.ref_policy, args.per_device_train_batch_size, args.fp16, args.bf16
            )
            self.deepspeed = self.model
        else:
            self.ref_policy = self.ref_policy.to(self.accelerator.device)
            if isinstance(self.reward_model, nn.Module):
                self.reward_model = self.reward_model.to(self.accelerator.device)

        # Create optimizer if not passed in
        if self.optimizer is None:
            self.create_optimizer_and_scheduler(num_training_steps=args.num_total_batches)

        # Setup HF Trainer state + callbacks
        default_callbacks = DEFAULT_CALLBACKS + get_reporting_integration_callbacks(self.args.report_to)
        self.callbacks = default_callbacks if callbacks is None else default_callbacks + callbacks
        self.callback_handler = CallbackHandler(
            self.callbacks, self.model, self.processing_class, self.optimizer, self.lr_scheduler
        )
        self.add_callback(PrinterCallback if self.args.disable_tqdm else DEFAULT_PROGRESS_CALLBACK)


        self.control = TrainerControl()
        self.state = OnlineTrainerState(
            is_local_process_zero=self.is_local_process_zero(),
            is_world_process_zero=self.is_world_process_zero(),
            stateful_callbacks=[
                cb for cb in self.callback_handler.callbacks + [self.control] if isinstance(cb, ExportableState)
            ],
            save_steps=args.save_steps
        )
        self.current_flos = 0
        self.hp_search_backend = None

        # Create local dir, push to hub, etc.
        if self.args.push_to_hub:
            self.init_hf_repo()
        if self.args.should_save:
            os.makedirs(self.args.output_dir, exist_ok=True)

        # Tag model if needed
        if hasattr(self.model, "add_model_tags"):
            self.model.add_model_tags(self._tag_names)

    def train(self):
        """
        Single-stage SCoRE training loop:
          1) Generate initial answer (and compute KL vs. ref policy).
          2) Generate correction (and get reward from reward_model).
          3) Final reward = reward(correction) - beta * KL(initial).
          4) REINFORCE update on the correctionâ€™s log-probs.
        """
        args = self.args
        accelerator = self.accelerator
        device = accelerator.device
        dataloader = self.dataloader

        # internal trainer states
        self.state.global_step = 0
        self.state.episode = 0
        self.state.max_steps = args.num_total_batches  # or something else
        self.state.num_train_epochs = args.num_train_epochs  # for logging

        # Start
        self.control = self.callback_handler.on_train_begin(args, self.state, self.control)
        start_time = time.time()

        # Reusable function to get next batch, infinitely
        def repeat_generator():
            while True:
                yield from dataloader

        iter_dataloader = iter(repeat_generator())
        self.model.train()

        # ================= [æ–°å¢ä»£ç å¼€å§‹] =================
        # æ‰‹åŠ¨æŒ‡å®šè¦æ¢å¤çš„ checkpoint è·¯å¾„å’Œæ­¥æ•°
        # resume_step = 300  # ä½ ä¿å­˜çš„ checkpoint æ­¥æ•°
        # resume_path = "/root/autodl-tmp/data/math500/cache/SCoRE/score_stage2/checkpoint-300" # è¯·ç¡®è®¤è·¯å¾„æ­£ç¡®
        
        # if resume_step > 0:
        #     print(f"ğŸ”¥ Resuming training from step {resume_step}...")
            
        #     # 1. åŠ è½½ Policy æƒé‡ (æ³¨æ„ï¼šåªåŠ è½½ç»™ self.modelï¼Œä¸è¦åŠ¨ self.ref_policy)
        #     # ä½ çš„ä»£ç ä½¿ç”¨äº† Acceleratorï¼Œæ‰€ä»¥æœ€å¥½é€šè¿‡ unwrap åŠ è½½ï¼Œæˆ–è€…ç›´æ¥åŠ è½½
        #     # è¿™é‡Œå‡è®¾ä¿å­˜çš„æ˜¯ safetensors æˆ– binï¼Œä¸”æ˜¯ LoRA é€‚é…å™¨
        #     from peft import PeftModel
        #     # é‡æ–°åŠ è½½é€‚é…å™¨æƒé‡
        #     self.model.load_adapter(resume_path, adapter_name="default")
            
        #     # 2. æ¢å¤çŠ¶æ€å˜é‡
        #     self.state.global_step = resume_step
        #     self.state.episode = resume_step * args.batch_size
            
        #     # æ³¨æ„ï¼šä¼˜åŒ–å™¨çŠ¶æ€(Optimizer State)åœ¨è¿™é‡Œå¾ˆéš¾å®Œç¾æ¢å¤ï¼Œ
        #     # å› ä¸ºä½ çš„ä»£ç æ˜¯åœ¨ train å†…éƒ¨åˆ›å»ºçš„ optimizerã€‚
        #     # æ¥å—â€œå­¦ä¹ ç‡é‡æ–°é¢„çƒ­â€é€šå¸¸æ˜¯å¯ä»¥æ¥å—çš„ã€‚
        # ================= [æ–°å¢ä»£ç ç»“æŸ] =================

        ema_stats = {} 
        ema_alpha = 0.05  # å¹³æ»‘ç³»æ•°

        self.processing_class.padding_side = "left"
        if self.processing_class.pad_token_id is None:
            self.processing_class.pad_token_id = self.processing_class.eos_token_id

        for step_idx in range(1, args.num_total_batches + 1):
            data = next(iter_dataloader)
            self.state.episode += args.batch_size

            # ------------------------
            # 1) Generate INITIAL and CORRECTION (é‡‡æ ·ç­–ç•¥æŒ‰é˜¶æ®µåŒºåˆ†)
            # ------------------------
            with torch.no_grad():
                queries = data["input_ids"].to(device).long()

                with unwrap_model_for_generation(
                    self.model, accelerator, gather_deepspeed3_params=args.ds3_gather_for_generation
                ) as unwrapped_model:
                    
                    # ======== å…³é”®ä¿®å¤ï¼šé‡‡æ ·ç­–ç•¥æŒ‰è®­ç»ƒé˜¶æ®µåŒºåˆ† ========
                    if args.stage == 1:
                        # Stage I: 100% ä½¿ç”¨ ref_policy (base model)
                        # ç›®æ ‡ï¼šå­¦ä¹ å¦‚ä½•çº æ­£ base model çš„æ¬¡ä¼˜ç­”æ¡ˆ
                        init_outputs, _ = batch_generation(
                            self.ref_policy,  # âœ“ ä¿®å¤ï¼šStage I å¿…é¡»ç”¨ ref_policy
                            queries,
                            args.local_rollout_forward_batch_size,
                            self.processing_class.pad_token_id,
                            self.init_generation_config,
                        )
                    else:
                        # Stage II: æ ·æœ¬çº§æ··åˆé‡‡æ ·ï¼ˆè®ºæ–‡ Section 5.3ï¼‰
                        # å¯¹æ¯ä¸ªæ ·æœ¬ç‹¬ç«‹å†³å®šä½¿ç”¨ ref_policy æˆ–å½“å‰ç­–ç•¥
                        # è¿™æ ·ç¡®ä¿æ¢¯åº¦æ›´æ–°çš„ç¨³å®šæ€§
                        batch_size = queries.shape[0]
                        use_offline_mask = torch.rand(batch_size, device=device) < args.offline_y1_ratio
                        
                        # åˆ†åˆ«å¯¹ä¸¤ç»„æ ·æœ¬ç”Ÿæˆ
                        offline_indices = torch.where(use_offline_mask)[0]
                        online_indices = torch.where(~use_offline_mask)[0]
                        
                        outputs_list = []
                        indices_list = []

                        if len(offline_indices) > 0:
                            offline_queries = queries[offline_indices]
                            offline_outputs, _ = batch_generation(
                                self.ref_policy,
                                offline_queries,
                                args.local_rollout_forward_batch_size,
                                self.processing_class.pad_token_id,
                                self.init_generation_config,
                            )
                            outputs_list.append(offline_outputs)
                            indices_list.append(offline_indices)
                        
                        if len(online_indices) > 0:
                            online_queries = queries[online_indices]
                            online_outputs, _ = batch_generation(
                                unwrapped_model,
                                online_queries,
                                args.local_rollout_forward_batch_size,
                                self.processing_class.pad_token_id,
                                self.init_generation_config,
                            )
                            outputs_list.append(online_outputs)
                            indices_list.append(online_indices)
                        
                        # Pad to same length before concatenation
                        if len(outputs_list) > 1:
                            max_len = max(x.shape[1] for x in outputs_list)
                            for i in range(len(outputs_list)):
                                if outputs_list[i].shape[1] < max_len:
                                    outputs_list[i] = torch.nn.functional.pad(
                                        outputs_list[i], 
                                        (0, max_len - outputs_list[i].shape[1]), 
                                        value=self.processing_class.pad_token_id
                                    )
                        
                        init_outputs = torch.cat(outputs_list, dim=0)
                        combined_indices = torch.cat(indices_list, dim=0)
                        sort_order = torch.argsort(combined_indices)
                        init_outputs = init_outputs[sort_order]

                    init_context_len = queries.shape[1]
                    init_answers = init_outputs[:, init_context_len:]

                    # ç”Ÿæˆ correction çš„è¾“å…¥
                    init_answer_texts = self.processing_class.batch_decode(init_answers, skip_special_tokens=True)
                    corr_inputs = build_correction_inputs_for_batch(
                        data, 
                        init_answer_texts,
                        self.processing_class,
                        self.prompt_builder,
                        question_col=self.algo_config['question_col'],
                    ).to(device)
                
                    # y2 (correction) å§‹ç»ˆä»å½“å‰è®­ç»ƒçš„ç­–ç•¥é‡‡æ ·
                    corr_outputs, _ = batch_generation(
                        unwrapped_model,  # Stage I/II éƒ½ä» self.model é‡‡æ · y2
                        corr_inputs,
                        args.local_rollout_forward_batch_size,
                        self.processing_class.pad_token_id,
                        self.init_generation_config,
                    )
                
                    corr_context_len = corr_inputs.shape[1]
                    corr_tokens = corr_outputs[:, corr_context_len:]

                # è®¡ç®— reference policy çš„ log prob (ç”¨äº KLï¼Œä¸éœ€è¦æ¢¯åº¦)
                ref_init_out = forward(self.ref_policy, init_outputs, self.processing_class.pad_token_id)
                ref_init_logits = ref_init_out.logits[:, init_context_len - 1 : -1]
                # ref_init_logits /= args.temperature + 1e-7
                ref_init_logprob = selective_log_softmax(ref_init_logits, init_answers)
                mask_init = (init_answers == self.processing_class.pad_token_id)
                ref_init_logprob = ref_init_logprob.masked_fill_(mask_init, 0)

                ref_corr_out = forward(self.ref_policy, corr_outputs, self.processing_class.pad_token_id)
                ref_corr_logits = ref_corr_out.logits[:, corr_context_len - 1 : -1]
                # ref_corr_logits /= args.temperature + 1e-7
                ref_corr_logprob = selective_log_softmax(ref_corr_logits, corr_tokens)
                mask_corr = (corr_tokens == self.processing_class.pad_token_id)
                ref_corr_logprob = ref_corr_logprob.masked_fill_(mask_corr, 0)

                del ref_init_out, ref_init_logits, ref_corr_out, ref_corr_logits
                torch.cuda.empty_cache()

            # è®¡ç®— reward
            corr_output_text = self.processing_class.batch_decode(corr_tokens, skip_special_tokens=True)
            with torch.no_grad():
                reward_corr = torch.tensor([
                    self.reward_model(model_answer=corr_output, ground_truth=reference)
                    for (corr_output, reference) in zip(corr_output_text, data[self.algo_config["gold_col"]])
                ], dtype=torch.float, device=device)

                reward_init = torch.tensor([
                    self.reward_model(model_answer=init_output, ground_truth=reference)
                    for (init_output, reference) in zip(init_answer_texts, data[self.algo_config["gold_col"]])
                ], dtype=torch.float, device=device)

            torch.cuda.empty_cache()
            gc.collect()

            # ------------------------
            # 3) è®¡ç®— Loss (ç¬¦åˆè®ºæ–‡å…¬å¼)
            # ------------------------
            # è®ºæ–‡ Stage I: max E[r(y2)] - Î²Â·KL(Ï€_Î¸(Â·|x1) || Ï€_ref(Â·|x1))
            # è®ºæ–‡ Stage II: max E[r(y1) + rÌƒ(y2)] - Î²Â·(KL_1 + KL_2)
            # 
            # å…³é”®ç‚¹ï¼šKL æ˜¯ç‹¬ç«‹çš„æ­£åˆ™åŒ–é¡¹ï¼Œéœ€è¦æœ‰æ¢¯åº¦ï¼
            # Policy gradient éƒ¨åˆ†ä½¿ç”¨ baseline å‡å‡å€¼

            micro_step = 0
            total_loss = 0
            # ç”¨äºæ—¥å¿—è®°å½• KLï¼ŒStage 1/2 å‡ç´¯ç§¯ä¸¤é¡¹
            total_kl_init = 0
            total_kl_corr = 0
            
            for micro_start in range(0, args.local_batch_size, args.per_device_train_batch_size):                
                micro_end = micro_start + args.per_device_train_batch_size
                mb_idx = slice(micro_start, micro_end)
                mb_corr_outputs = corr_outputs[mb_idx]
                mb_corr_tokens = corr_tokens[mb_idx]
                mb_init_tokens = init_answers[mb_idx]
                mb_init_outputs = init_outputs[mb_idx]
                mb_reward_corr = reward_corr[mb_idx]
                mb_reward_init = reward_init[mb_idx]
                mb_ref_init_logprob = ref_init_logprob[mb_idx]
                mb_ref_corr_logprob = ref_corr_logprob[mb_idx]
                mb_mask_init = mask_init[mb_idx]
                mb_mask_corr = mask_corr[mb_idx]
                mb_queries = queries[mb_idx]
                mb_init_context_len = mb_queries.shape[1]

                # ========== è®¡ç®—å½“å‰ç­–ç•¥çš„ log prob (éœ€è¦æ¢¯åº¦) ==========
                # Stage I å’Œ Stage II éƒ½éœ€è¦è®¡ç®— y1 å’Œ y2 çš„ logprob
                # åŒºåˆ«ï¼š
                # - Stage I: y1 çš„ logprob ä»…ç”¨äº KL çº¦æŸï¼Œä¸åš PG
                # - Stage II: y1 çš„ logprob ç”¨äº KL çº¦æŸ + PG
                
                # y1 çš„ log probï¼ˆStage I/II éƒ½éœ€è¦ï¼Œç”¨äº KL çº¦æŸï¼‰
                out_init = forward(self.model, mb_init_outputs, self.processing_class.pad_token_id)
                logits_init = out_init.logits[:, mb_init_context_len - 1 : -1]
                logprob_init = selective_log_softmax(logits_init, mb_init_tokens)
                logprob_init = logprob_init.masked_fill_(mb_mask_init, 0)
                
                # y2 çš„ log probï¼ˆStage I/II éƒ½éœ€è¦ï¼‰
                out_corr = forward(self.model, mb_corr_outputs, self.processing_class.pad_token_id)
                logits_corr = out_corr.logits[:, corr_context_len - 1 : -1]
                logprob_corr = selective_log_softmax(logits_corr, mb_corr_tokens)
                logprob_corr = logprob_corr.masked_fill_(mb_mask_corr, 0)

                # ========== è®¡ç®— Policy Gradient Loss ==========
                sum_lp_corr = logprob_corr.sum(dim=1)

                if args.stage == 1:
                    # ============================================================
                    # Stage I: åˆå§‹åŒ–é˜¶æ®µï¼ˆè®ºæ–‡ Section 4.1 + 5.1ï¼‰
                    # ============================================================
                    # å®Œæ•´ç›®æ ‡: max E[r(y2)] - Î²1Â·D_KL(Ï€_Î¸(y1|x) || Ï€_ref(y1|x)) 
                    #                          - Î²2Â·D_KL(Ï€_Î¸(y2|x,y1) || Ï€_ref(y2|x,y1))
                    # 
                    # å…³é”®ç‚¹ï¼š
                    # 1. y1 æ¥è‡ª ref_policyï¼ˆç¦»çº¿æ•°æ®ï¼‰ï¼Œä¸åš Policy Gradient
                    # 2. ä½†éœ€è¦ KL çº¦æŸï¼Œç¡®ä¿æ¨¡å‹åœ¨ç”Ÿæˆ y1 æ—¶ä¿æŒæ¥è¿‘ base model
                    # 3. y2 åš Policy Gradient + KL çº¦æŸï¼Œå­¦ä¹ çº é”™èƒ½åŠ›
                    # 4. åŒé‡ KL çº¦æŸé˜²æ­¢æ¨¡å‹å´©æºƒåˆ°"ç›´æ¥è§£å†³æ–¹æ¡ˆ"
                    # ============================================================
                    
                    with torch.no_grad():
                        baseline_corr = reward_corr.mean()
                        advantage_corr = (mb_reward_corr - baseline_corr)

                    # Policy Gradient: åªä¼˜åŒ– y2
                    sum_lp_corr = logprob_corr.sum(dim=1)
                    loss_pg_corr = -(advantage_corr * sum_lp_corr).mean()

                    # KL æ­£åˆ™åŒ–: çº¦æŸ y1 å’Œ y2
                    valid_mask_init = (~mb_mask_init).float()
                    valid_mask_corr = (~mb_mask_corr).float()
                    
                    # y1 çš„ KL: ç¡®ä¿æ¨¡å‹ç”Ÿæˆ y1 æ—¶æ¥è¿‘ base model
                    kl_init_per_token = (logprob_init - mb_ref_init_logprob) * valid_mask_init
                    len_init = valid_mask_init.sum(dim=1).clamp(min=1.0)
                    kl_init_per_sample = kl_init_per_token.sum(dim=1) / len_init
                    kl_init_per_sample = torch.clamp(kl_init_per_sample, min=0.0)
                    loss_kl_init = args.init_kl_coef * kl_init_per_sample.mean()
                    
                    # y2 çš„ KL: ç¡®ä¿çº é”™æ—¶ä¸è¿‡åº¦åç¦»
                    kl_corr_per_token = (logprob_corr - mb_ref_corr_logprob) * valid_mask_corr
                    len_corr = valid_mask_corr.sum(dim=1).clamp(min=1.0)
                    kl_corr_per_sample = kl_corr_per_token.sum(dim=1) / len_corr
                    kl_corr_per_sample = torch.clamp(kl_corr_per_sample, min=0.0)
                    loss_kl_corr = args.corr_kl_coef * kl_corr_per_sample.mean()

                    # Stage I æ€»æŸå¤±: PG(y2) + KL(y1) + KL(y2)
                    loss = loss_pg_corr + loss_kl_init + loss_kl_corr
                    
                    # æ—¥å¿—è®°å½•
                    total_kl_init += kl_init_per_sample.detach().mean().item()
                    total_kl_corr += kl_corr_per_sample.detach().mean().item()

                else:
                    # ============================================================
                    # Stage II: è”åˆä¼˜åŒ–é˜¶æ®µï¼ˆè®ºæ–‡ Section 4.2ï¼‰
                    # ============================================================
                    # è®ºæ–‡å…¬å¼: max E[r(y1) + rÌƒ(y2)] - Î²Â·(KL_1 + KL_2)
                    # å…¶ä¸­ rÌƒ(y2) = r(y2) + Î±Â·(r(y2) - r(y1))
                    # 
                    # å…³é”®ç†è§£ï¼š
                    # "è”åˆä¼˜åŒ–"æ˜¯æŒ‡æ•´ä½“ç›®æ ‡å‡½æ•°åŒ…å«ä¸¤é¡¹å¥–åŠ±
                    # ä½† REINFORCE æ¢¯åº¦åº”è¯¥åˆ†ç¦»ï¼š
                    # - y1 çš„æ¢¯åº¦ç”± r(y1) é©±åŠ¨
                    # - y2 çš„æ¢¯åº¦ç”± rÌƒ(y2) é©±åŠ¨
                    # ============================================================
                    sum_lp_init = logprob_init.sum(dim=1)
                    
                    # è®¡ç®— y2 çš„å¢å¼ºå¥–åŠ±ï¼ˆå¸¦ reward bonusï¼‰
                    r2_tilde = mb_reward_corr + args.stage2_alpha * (mb_reward_corr - mb_reward_init)
                    
                    with torch.no_grad():
                        # ä½¿ç”¨æ•´ä¸ª local batch è®¡ç®— baselineï¼ˆå‡å°‘æ–¹å·®ï¼‰
                        baseline_init = reward_init.mean()
                        
                        all_r2_tilde = reward_corr + args.stage2_alpha * (reward_corr - reward_init)
                        baseline_corr = all_r2_tilde.mean()
                        
                        # åˆ†ç¦»çš„ advantage
                        advantage_init = mb_reward_init - baseline_init  # y1 ä¼˜åŒ–è‡ªèº«å‡†ç¡®ç‡
                        advantage_corr = r2_tilde - baseline_corr        # y2 ä¼˜åŒ–çº é”™+bonus
                    
                    # Policy Gradient: åˆ†ç¦»ä¼˜åŒ–
                    # y1: æœ€å¤§åŒ–é¦–è½®å‡†ç¡®ç‡
                    loss_pg_init = -(advantage_init * sum_lp_init).mean()
                    # y2: æœ€å¤§åŒ–çº é”™å‡†ç¡®ç‡ + çº é”™å¢ç›Šå¥–åŠ±
                    loss_pg_corr = -(advantage_corr * sum_lp_corr).mean()
                    
                    # KL æ­£åˆ™åŒ–ï¼ˆé•¿åº¦å½’ä¸€åŒ–ç‰ˆæœ¬ï¼Œä¸ Stage I ä¿æŒä¸€è‡´ï¼‰
                    valid_mask_init = (~mb_mask_init).float()
                    valid_mask_corr = (~mb_mask_corr).float()
                    
                    kl_init_per_token = (logprob_init - mb_ref_init_logprob) * valid_mask_init
                    kl_corr_per_token = (logprob_corr - mb_ref_corr_logprob) * valid_mask_corr
                    
                    len_init = valid_mask_init.sum(dim=1).clamp(min=1.0)
                    len_corr = valid_mask_corr.sum(dim=1).clamp(min=1.0)
                    
                    kl_init_per_sample = kl_init_per_token.sum(dim=1) / len_init
                    kl_corr_per_sample = kl_corr_per_token.sum(dim=1) / len_corr
                    
                    kl_init_per_sample = torch.clamp(kl_init_per_sample, min=0.0)
                    kl_corr_per_sample = torch.clamp(kl_corr_per_sample, min=0.0)
                    
                    # KL æŸå¤±: Î²Â·(KL_1 + KL_2)
                    loss_kl_init = args.init_kl_coef * kl_init_per_sample.mean()
                    loss_kl_corr = args.corr_kl_coef * kl_corr_per_sample.mean()
                    loss_kl = loss_kl_init + loss_kl_corr
                    
                    # æ€»æŸå¤±: è®ºæ–‡å…¬å¼çš„è´Ÿæ•°å½¢å¼
                    # -max E[r(y1) + rÌƒ(y2)] + Î²Â·(KL_1 + KL_2)
                    # = min -E[r(y1)] - E[rÌƒ(y2)] + Î²Â·(KL_1 + KL_2)
                    loss = loss_pg_init + loss_pg_corr + loss_kl
                    
                    # è®°å½•ç”¨äºæ—¥å¿—
                    total_kl_init += kl_init_per_sample.detach().mean().item()
                    total_kl_corr += kl_corr_per_sample.detach().mean().item()
                loss = loss / self.algo_config['gradient_accumulation_steps']
                total_loss += loss.item()

                accelerator.backward(loss)
                micro_step += 1

                if micro_step == self.algo_config['gradient_accumulation_steps']:
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=0.5)
                    self.optimizer.step()
                    self.lr_scheduler.step()
                    self.optimizer.zero_grad()
                    micro_step = 0

            # ------------------------
            # Logging / stats
            # ------------------------
            with torch.no_grad():
                mean_reward_corr = accelerator.gather_for_metrics(reward_corr).mean().item()
                mean_reward_init = accelerator.gather_for_metrics(reward_init).mean().item()
                mean_reward_delta = mean_reward_corr - mean_reward_init
                if 'reward_corr' not in ema_stats:
                    ema_stats['reward_corr'] = mean_reward_corr
                    ema_stats['reward_delta'] = mean_reward_delta
                    ema_stats['kl_corr'] = total_kl_corr / max(1, args.local_batch_size // args.per_device_train_batch_size)
                else:
                    # åªæœ‰å½“å­—å…¸é‡Œå·²ç»æœ‰å€¼äº†ï¼Œæ‰è¿›è¡Œå¹³æ»‘æ›´æ–°
                    curr_kl = total_kl_corr / max(1, args.local_batch_size // args.per_device_train_batch_size)
                    ema_stats['reward_corr'] = (1 - ema_alpha) * ema_stats['reward_corr'] + ema_alpha * mean_reward_corr
                    ema_stats['reward_delta'] = (1 - ema_alpha) * ema_stats['reward_delta'] + ema_alpha * mean_reward_delta
                    ema_stats['kl_corr'] = (1 - ema_alpha) * ema_stats['kl_corr'] + ema_alpha * curr_kl

                metrics = {}
                metrics["score/kl_init"] = total_kl_init / max(1, args.local_batch_size // args.per_device_train_batch_size)
                metrics["score/kl_corr"] = total_kl_corr / max(1, args.local_batch_size // args.per_device_train_batch_size)
                metrics["score/reward_init"] = mean_reward_init
                metrics["score/reward_corr"] = mean_reward_corr
                metrics["score/reward_delta"] = mean_reward_corr - mean_reward_init  # å…³é”®æŒ‡æ ‡ï¼šçº é”™æå‡
                metrics["loss"] = total_loss
                metrics["episode"] = self.state.episode
                metrics["step"] = step_idx
                metrics["score_ema/reward_corr"] = ema_stats['reward_corr']
                metrics["score_ema/reward_delta"] = ema_stats['reward_delta']
                metrics["score_ema/kl_corr"] = ema_stats['kl_corr']
                self.log(metrics)

            del corr_outputs, corr_tokens, init_outputs, init_answers, queries
            del out_corr, out_init, logits_corr, logits_init, logprob_corr, logprob_init
            del ref_init_logprob, ref_corr_logprob, mask_init, mask_corr
            torch.cuda.empty_cache()
            gc.collect()

            self.state.global_step += 1
            self.control = self.callback_handler.on_step_end(args, self.state, self.control)
            if self.control.should_save:
                self._save_checkpoint(self.model, trial=None)
                self.control = self.callback_handler.on_save(self.args, self.state, self.control)

            # if (
            #     args.num_sample_generations > 0
            #     and (step_idx - 1) % max(1, args.num_total_batches // args.num_sample_generations) == 0
            # ):
            #     self.generate_completions(sampling=True)

            # Early termination if needed
            if self.control.should_training_stop:
                break

        self.control = self.callback_handler.on_train_end(args, self.state, self.control)
        if self.control.should_save:
            self._save_checkpoint(self.model, trial=None)
            self.control = self.callback_handler.on_save(self.args, self.state, self.control)

        print("SCoRE training completed!")

    def generate_completions(self, sampling: bool = False):
        """
        Utility function to sample model completions on `eval_dataloader` and log them.
        Copied from RLOOTrainer's style, but simplified.
        """

        raise NotImplementedError('Not Yet')

        # args = self.args
        # if self.eval_dataloader is None:
        #     return

        # generation_config = GenerationConfig(
        #     max_new_tokens=args.response_length,
        #     temperature=(0.01 + 1e-7),
        #     top_k=0.0,
        #     top_p=1.0,
        #     do_sample=True,
        # )

        # table = defaultdict(list)
        # with unwrap_model_for_generation(
        #     self.model, self.accelerator, gather_deepspeed3_params=args.ds3_gather_for_generation
        # ) as unwrapped_model:
        #     for batch in self.eval_dataloader:
        #         query = batch["input_ids"]
        #         with torch.no_grad():
        #             context_length = query.shape[1]
        #             query_response, _ = batch_generation(
        #                 unwrapped_model,
        #                 query,
        #                 query.shape[0],
        #                 self.processing_class.pad_token_id,
        #                 generation_config,
        #             )
        #             response = query_response[:, context_length:]
        #             table["query"].extend(
        #                 gather_object(self.processing_class.batch_decode(query, skip_special_tokens=True))
        #             )
        #             table["model response"].extend(
        #                 gather_object(self.processing_class.batch_decode(response, skip_special_tokens=True))
        #             )

        #         if sampling:
        #             # Just do one batch if sampling
        #             break

        # df = pd.DataFrame(table)
        # if self.accelerator.is_main_process:
        #     print_rich_table(df.iloc[0 : 0 + 5])
        #     # If using W&B or Comet, you can log the table
        #     # ...
    
    def create_model_card(
        self,
        model_name: Optional[str] = None,
        dataset_name: Optional[str] = None,
        tags: Union[str, list[str], None] = None,
    ):
        """
        Creates a draft of a model card using the information available to the `Trainer`.

        Args:
            model_name (`str` or `None`, *optional*, defaults to `None`):
                Name of the model.
            dataset_name (`str` or `None`, *optional*, defaults to `None`):
                Name of the dataset used for training.
            tags (`str`, `list[str]` or `None`, *optional*, defaults to `None`):
                Tags to be associated with the model card.
        """
        if not self.is_world_process_zero():
            return

        if hasattr(self.model.config, "_name_or_path") and not os.path.isdir(self.model.config._name_or_path):
            base_model = self.model.config._name_or_path
        else:
            base_model = None

        tags = tags or []
        if isinstance(tags, str):
            tags = [tags]

        if hasattr(self.model.config, "unsloth_version"):
            tags.append("unsloth")

        try:
            import wandb
            from wandb import run as wandb_run
            wandb_url = wandb_run.get_url() if wandb_run is not None else None
        except ImportError:
            wandb_url = None

        model_card = generate_model_card(
            base_model=base_model,
            model_name=model_name,
            hub_model_id=self.hub_model_id,
            dataset_name=dataset_name,
            tags=tags,
            wandb_url=wandb_url,
            comet_url=get_comet_experiment_url(),
            trainer_name="SCoRE",
        )

        model_card.save(os.path.join(self.args.output_dir, "README.md"))



def build_correction_inputs_for_batch(
    batch,
    init_answer_texts,
    tokenizer,
    prompt_builder,
    question_col: str = "question",
    initial_answer_col: str = "initial_answer",
):
    # We will store the final "correction input" for each row
    batch_correction_inputs = []

    for i, init_ans_text in enumerate(init_answer_texts):
        question_text = batch[question_col][i]

        # Build a 'sample' dict as your prompt_builder expects:
        sample_for_prompt = {
            question_col: question_text,
            initial_answer_col: [init_ans_text],  # your builder uses a list for initial answers
        }

        # Now get the final correction prompt(s). Typically there's 1 prompt
        # in this scenario, but build_correction_prompt returns a list.
        corr_inputs = prompt_builder.build_correction_prompt(
            sample=sample_for_prompt,
            tokenizer=tokenizer,
            question_col=question_col,
            initial_answer_col=initial_answer_col,
            tokenize=True
        )
        # Since we used only 1 initial answer, corr_prompts[0] is the final text
        corr_inputs = corr_inputs[0]

        batch_correction_inputs.append({'input_ids': corr_inputs})


    collated_corrections = DataCollatorWithPadding(tokenizer)(batch_correction_inputs)
    return collated_corrections['input_ids']
