
# Model Configuration
model_path: "Qwen/Qwen2.5-Math-1.5B-Instruct" #'microsoft/Phi-3-mini-4k-instruct' # "Qwen/Qwen2.5-1.5B-Instruct" # "Qwen/Qwen2.5-Math-1.5B-Instruct"
cache_dir: "/root/autodl-tmp/data/math500/cache/"
random_seed: 42
wandb_project_name: 'SCoRE'

# Dataset Configuration 
task_type: "math" # "qa" or "math"
data_path: "data/math500"
id_col: "unique_id"                        # Unique identifier column in the dataset. "question_id" for QA, "problem" for "unique_id"
question_col: "problem"            # Column containing the question text. "question_text" for QA, "problem" for math500
gold_col: "answer"                   # Reference/GT answer column. "reference" for QA, "answer" for math500

run_name: "score_stage1"

# Reward Function
evaluator_mode: 'final'   # "default" will input all answer in reward func, "final" only string after "evaluator_answer_marker"
evaluator_function: 'math_acc' # metric function to use, in_acc, f1, em for QA, "math_acc" for math500. math500 always has "final" evaluation mode
evaluator_answer_marker: 'Final Answer: The final answer is' # the key word to find answer after. truncates answer untils this phrase. Case insensetive. 
# "Final Answer: The final answer is" for math
# "Final Answer:" for qa
 
# Generation Configuration
few_shot_dir: "few_shots"            # Directory containing few-shot JSON files
number_output_initial_generations: 1         

# Sampling Parameters
temperature: 1.0                   # Sampling temperature
max_tokens: 512              # Maximum number of tokens to generate

#显存要求29GB
# total batch = per_device_train_batch_size * num_gpus * gradient_accumulation_steps
train_stage: 1
per_device_train_batch_size: 2
gradient_accumulation_steps: 4
local_rollout_forward_batch_size: 4
total_episodes: 12000 # number of train samples seen. number of gradient steps = total_episodes / total batch 
learning_rate: 5.0e-5
num_warmup_steps: 100 # number of warmup steps in scheduler
# 根据论文 Table 8 MATH 任务的超参数设置
# Stage I: "Constrain to base model" (y1) + "Maximize reward" (y2)
# β2 用于约束 y1（first attempt）保持接近 base model
init_kl_coef: 0.1    # β2 = 0.1 (用于 y1 的 KL 约束，论文 Table 8)

save_steps: 100 # save every <N> steps
stage2_alpha: 10      # Stage I 不使用此参数，Stage II 使用 α = 10 (论文 Table 8)
offline_y1_ratio: 0.0  # Stage I 不使用此参数（始终从 ref_policy 采样）

use_lora: True
lora_rank: 32
lora_alpha: 64
lora_dropout: 0.05

